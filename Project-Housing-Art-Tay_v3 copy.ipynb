{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Kaggle Housing Price Prediction Challenge\"\n",
    "author: \"Art Tay\"\n",
    "format:\n",
    "  pdf:\n",
    "   documentclass: article\n",
    "   papersize: letter\n",
    "\n",
    "execute:\n",
    "  enabled: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
       "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
       "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
       "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
       "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
       "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
       "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
       "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
       "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
       "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
       "       'SaleCondition', 'SalePrice'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data\n",
    "train_raw = pd.read_csv(\"data/train.csv\")\n",
    "test_raw = pd.read_csv(\"data/test.csv\")\n",
    "train = train_raw\n",
    "train_raw.columns\n",
    "#test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "# sklearn transformers\n",
    "from sklearn.preprocessing \\\n",
    "    import StandardScaler, SplineTransformer, PowerTransformer, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts all object (string) columns to \n",
    "# be categorical.\n",
    "# @param: train - data in its raw form\n",
    "# @return: a pandas data frame with objects coded as categorical \n",
    "def to_cat(train):\n",
    "    train[train.select_dtypes(['object']).columns] = (\n",
    "        train.select_dtypes(['object'])\n",
    "        .apply(lambda x: x.astype('category'))\n",
    "    )\n",
    "    return train\n",
    "\n",
    "# Codes ad-hoc features to be categorical if they appear numeric\n",
    "# in the raw format. \n",
    "# @param: train - data in its raw form\n",
    "# @return: a pandas data frame with some columns marked as categorical.\n",
    "def some_num_to_cat(train): \n",
    "    train['MSSubClass'] = train['MSSubClass'].astype('category')\n",
    "    train['YearBuilt'] = train['YearBuilt'].astype('category')\n",
    "    train['YearRemodAdd'] = train['YearRemodAdd'].astype('category')\n",
    "    train['GarageYrBlt'] = train['GarageYrBlt'].astype('category')\n",
    "    train['MoSold'] = train['MoSold'].astype('category')\n",
    "    train['YrSold'] = train['YrSold'].astype('category')\n",
    "\n",
    "    return train\n",
    "\n",
    "# Engineering pre-spec features\n",
    "# @param: train - data in its raw form \n",
    "# @return: a pandas data frame with added features\n",
    "def feat_eng(train): \n",
    "    # Feature Engineering\n",
    "    # NewGarage\n",
    "    train['NewGarage'] = (\n",
    "        np.where(train['GarageYrBlt'].isnull(), 0, \n",
    "            np.where(train['GarageYrBlt'] > train['YearBuilt'], 1, 0))\n",
    "    )\n",
    "    train['NewGarage'] = train['NewGarage'].astype('category')\n",
    "\n",
    "    # YearSinceRmdl\n",
    "    train['YearSinceRmdl'] = 2016 - train['YearRemodAdd']\n",
    "\n",
    "    # Rmdl\n",
    "    train['Rmdl'] = np.where(\n",
    "            train['YearBuilt'] < train['YearRemodAdd'], 1, 0)\n",
    "    train['Rmdl'] = train['Rmdl'].astype('category')\n",
    "\n",
    "    # TotalPorchArea\n",
    "    train['TotalPorchArea'] = (\n",
    "        train['WoodDeckSF'] + train['OpenPorchSF'] + \n",
    "        train['EnclosedPorch'] + train['3SsnPorch'] + \n",
    "        train['ScreenPorch']\n",
    "    )\n",
    "\n",
    "    #PorchYes\n",
    "    train['PorchYes'] = np.where(train['TotalPorchArea'] > 0, 1, 0)\n",
    "    train['PorchYes'] = train['PorchYes'].astype('category')\n",
    "\n",
    "    # TotalFinishedBsmt\n",
    "    train['TotalFinishedBsmt'] = train['BsmtFinSF1'] + train['BsmtFinSF2']\n",
    "\n",
    "    # PercentFinishedBsmt\n",
    "    train['PercentFinishedBsmt'] = np.where(train['TotalBsmtSF'] > 0, \n",
    "        train['TotalFinishedBsmt'] / train['TotalBsmtSF'] * 100, 0)\n",
    "\n",
    "    # TotalSqFt\n",
    "    train['TotalSqFt'] = train['GrLivArea'] + train['TotalFinishedBsmt']\n",
    "\n",
    "    # PercentLowQual\n",
    "    train['PercentLowQual'] = train['LowQualFinSF'] * 100 / train['TotalSqFt']\n",
    "\n",
    "    # IsNew\n",
    "    train['IsNew'] = np.where(\n",
    "        train['YrSold'] == train['YearRemodAdd'], 1, 0)\n",
    "    train['IsNew'] = train['IsNew'].astype('category')\n",
    "\n",
    "    # House_Age\n",
    "    train['House_age'] = train['YrSold'] - train['YearRemodAdd']\n",
    "\n",
    "    # NeighRich\n",
    "    train['NeighRich'] = np.select(\n",
    "        condlist = [\n",
    "            train['Neighborhood'] == ('StoneBr' or 'NridgHt' or 'NoRidge'), \n",
    "            train['Neighborhood'] == ('MeadowV' or 'IDOTRR' or 'BrDale')\n",
    "        ], \n",
    "        choicelist = [2, 0],\n",
    "        default = 1\n",
    "    )\n",
    "    train['NeighRich'] = train['NeighRich'].astype('category')\n",
    "    \n",
    "    return train\n",
    "\n",
    "# A helper function that converts a column to an ordinal scale.\n",
    "# Scale was determined ad-hoc.\n",
    "# @param: train - data in its raw form \n",
    "# @param: col_name - a string name of the column to be converted\n",
    "def ord_scale_1(train, col_name):\n",
    "    ret = np.select(\n",
    "        condlist = [\n",
    "            train[col_name] == \"Ex\", \n",
    "            train[col_name] == \"Gd\", \n",
    "            train[col_name] == \"TA\", \n",
    "            train[col_name] == \"Fa\", \n",
    "            train[col_name] == \"Po\"\n",
    "        ], \n",
    "        choicelist = [5, 4, 3, 2, 1], \n",
    "        default = 0\n",
    "    )\n",
    "    return ret\n",
    "\n",
    "def ord_scale_2(train, col_name):\n",
    "    ret = np.select(\n",
    "        condlist = [\n",
    "            train[col_name] == \"GLQ\", \n",
    "            train[col_name] == \"ALQ\", \n",
    "            train[col_name] == \"BLQ\", \n",
    "            train[col_name] == \"REC\", \n",
    "            train[col_name] == \"LwQ\", \n",
    "            train[col_name] == \"Unf\", \n",
    "        ], \n",
    "        choicelist = [6, 5, 4, 3, 2, 1], \n",
    "        default = 0\n",
    "    )\n",
    "    return ret\n",
    "\n",
    "def ord_encode(train): \n",
    "    # Ordinal Scale 1\n",
    "    cols_scale_1 = ['ExterQual', 'ExterCond', 'HeatingQC', 'KitchenQual', \n",
    "                    'BsmtQual', 'BsmtCond', 'FireplaceQu', 'GarageQual', \n",
    "                    'GarageCond', 'PoolQC']\n",
    "    \n",
    "    for i in cols_scale_1:\n",
    "        train[i] = ord_scale_1(train, i)\n",
    "\n",
    "    # Ordinal Scale 2 \n",
    "    train['BsmtFinType1'] = ord_scale_2(train, 'BsmtFinType1')\n",
    "    train['BsmtFinType2'] = ord_scale_2(train, 'BsmtFinType2')\n",
    "\n",
    "    # Ad-hoc ordeal scales \n",
    "    train['LotShape'] = np.select(\n",
    "        condlist = [\n",
    "            train['LotShape'] == \"Reg\", \n",
    "            train['LotShape'] == \"IR1\", \n",
    "            train['LotShape'] == \"IR2\", \n",
    "            train['LotShape'] == \"IR3\" \n",
    "        ], \n",
    "        choicelist = [3, 2, 1, 0]\n",
    "    )\n",
    "\n",
    "    train['LandSlope'] = np.select(\n",
    "        condlist = [\n",
    "            train['LandSlope'] == \"Gtl\", \n",
    "            train['LandSlope'] == \"Mod\", \n",
    "            train['LandSlope'] == \"Sev\"\n",
    "        ], \n",
    "        choicelist = [2, 1, 0]\n",
    "    )\n",
    "\n",
    "    train['BsmtExposure'] = np.select(\n",
    "        condlist = [\n",
    "            train['BsmtExposure'] == \"Gd\", \n",
    "            train['BsmtExposure'] == \"Av\", \n",
    "            train['BsmtExposure'] == \"Mn\", \n",
    "            train['BsmtExposure'] == \"No\"\n",
    "        ], \n",
    "        choicelist = [4, 3, 2, 1], \n",
    "        default = 0\n",
    "    )\n",
    "\n",
    "    train['GarageFinish'] = np.select(\n",
    "        condlist = [\n",
    "            train['GarageFinish'] == \"Fin\", \n",
    "            train['GarageFinish'] == \"RFn\", \n",
    "            train['GarageFinish'] == \"Unf\", \n",
    "        ], \n",
    "        choicelist = [3, 2, 1], \n",
    "        default = 0\n",
    "    )\n",
    "\n",
    "    train['Functional'] = np.select(\n",
    "        condlist = [\n",
    "            train['Functional'] == \"Typ\", \n",
    "            train['Functional'] == \"Min1\", \n",
    "            train['Functional'] == \"Min2\", \n",
    "            train['Functional'] == \"Mod\", \n",
    "            train['Functional'] == \"Maj1\", \n",
    "            train['Functional'] == \"Maj2\", \n",
    "            train['Functional'] == \"Sev\", \n",
    "            train['Functional'] == \"Sal\" \n",
    "        ], \n",
    "        choicelist = [7, 6, 5, 4, 3, 2, 1, 0]\n",
    "    )\n",
    "\n",
    "    return train\n",
    "\n",
    "def knn_Impute(train, numeric_cols, cat_cols, neighbors = 5, \n",
    "                reverse_scale = True, reverse_dummy = True):\n",
    "    # Scale the numeric columns\n",
    "    scaler = StandardScaler()\n",
    "    scaled_values = scaler.fit_transform(train[numeric_cols])\n",
    "    train = train.drop(numeric_cols, axis = 1)\n",
    "    train = train.join(pd.DataFrame(scaled_values, columns = numeric_cols))\n",
    "\n",
    "    # Dummy the categorical columns \n",
    "    dummy = OneHotEncoder(drop = 'first')\n",
    "    dummy_values = dummy.fit_transform(train[cat_cols]).toarray()\n",
    "    dummy_names = dummy.get_feature_names_out().tolist()\n",
    "    train = train.drop(cat_cols, axis = 1)\n",
    "    train = train.join(pd.DataFrame(dummy_values, columns = dummy_names))\n",
    "\n",
    "    # Knn imputation\n",
    "    imputer = KNNImputer(n_neighbors = neighbors)\n",
    "    train = pd.DataFrame(imputer.fit_transform(train), columns = train.columns)\n",
    "\n",
    "    #print(train.head())\n",
    "\n",
    "    # Reverse scaling\n",
    "    if reverse_scale: \n",
    "        no_scale_values = scaler.inverse_transform(train[numeric_cols])\n",
    "        train = train.drop(numeric_cols, axis = 1)\n",
    "        train = train.join(pd.DataFrame(no_scale_values, columns = numeric_cols))\n",
    "\n",
    "    # Reverse dummies\n",
    "    if reverse_dummy: \n",
    "        no_dummy_values = dummy.inverse_transform(train[dummy_names]) \n",
    "        train = train.drop(dummy_names, axis = 1)\n",
    "        train = train.join(pd.DataFrame(no_dummy_values, columns = cat_cols))\n",
    "\n",
    "    # Reversal of dummy makes them objects again\n",
    "    return to_cat(train)\n",
    "\n",
    "def dummy_cols(train, cat_cols, drop_first = True): \n",
    "    if drop_first:\n",
    "        dummy = OneHotEncoder(drop = 'first')\n",
    "    else:\n",
    "        dummy = OneHotEncoder()\n",
    "    dummy_values = dummy.fit_transform(train[cat_cols]).toarray()\n",
    "    dummy_names = dummy.get_feature_names_out().tolist()\n",
    "    train = train.drop(cat_cols, axis = 1)\n",
    "    train = train.join(pd.DataFrame(dummy_values, columns = dummy_names))\n",
    "\n",
    "    return train\n",
    "\n",
    "def drop_nzv(train, threshold = 0.05): \n",
    "    selector = VarianceThreshold(threshold = threshold)\n",
    "    train = train.loc[:, selector.fit(train).get_support()]\n",
    "\n",
    "    return train\n",
    "\n",
    "def yeo_johnson(train, numeric_cols, standardize = False):\n",
    "    yj = PowerTransformer(standardize = standardize)\n",
    "    yj_values = yj.fit_transform(train[numeric_cols])\n",
    "    train = train.drop(numeric_cols, axis = 1)\n",
    "    train = train.join(pd.DataFrame(yj_values, columns = numeric_cols))\n",
    "\n",
    "    return train\n",
    "\n",
    "def standardize(train, numeric_cols):\n",
    "    scaler = StandardScaler()\n",
    "    scaled_values = scaler.fit_transform(train[numeric_cols])\n",
    "    train = train.drop(numeric_cols, axis = 1)\n",
    "    train = train.join(pd.DataFrame(scaled_values, columns = numeric_cols))\n",
    "\n",
    "    return train\n",
    "\n",
    "def add_ns_3(train, cols, degree = 3, knots = 2): \n",
    "    spliner = SplineTransformer(degree = degree, n_knots = knots, include_bias = False)\n",
    "\n",
    "    for i in cols:\n",
    "        x = train[i].values.reshape(-1, 1)\n",
    "        new_col_names = [(i + \"_ns\" + str(j)) for j in range(1, degree + 1)]\n",
    "        spline = pd.DataFrame(spliner.fit_transform(x), columns = new_col_names)\n",
    "        train = train.join(spline)\n",
    "        train = train.drop(i, axis = 1)\n",
    "    \n",
    "    return train\n",
    "\n",
    "def drop_high_cor(df, threshold = 0.9):\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = df.corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Find features with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "    # Drop features \n",
    "    return df.drop(to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalized Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recipe_pLR(train, verbose = True, rep = True): \n",
    "    if verbose: print('Preprocessing Steps: ')\n",
    "    # Drop Id\n",
    "    if 'Id' in train: \n",
    "        train_Id = train['Id'] \n",
    "        train = train.drop('Id', axis = 1)\n",
    "\n",
    "    # Drop Response\n",
    "    if 'SalePrice' in train: \n",
    "        train_rep = train['SalePrice']\n",
    "        train = train.drop('SalePrice', axis = 1)\n",
    "\n",
    "    # Add user features\n",
    "    train = feat_eng(train)\n",
    "    if verbose: print(\"1. Added user engineered features\")\n",
    "\n",
    "    # Marks columns as categorical \n",
    "    train = to_cat(train)\n",
    "    train = some_num_to_cat(train)\n",
    "    if verbose: print('2. Encoded user specified variables as categorical')\n",
    "\n",
    "    # Ordinarily encodes select variables\n",
    "    train = ord_encode(train)\n",
    "    if verbose: print(\"3. Encoded user specified variables to be ordinal\")\n",
    "\n",
    "    # Track which variables are numeric and categorical \n",
    "    numeric_cols = train.select_dtypes(include = np.number).columns\n",
    "    cat_cols = train.select_dtypes('category').columns\n",
    "\n",
    "    # Imputes missing values\n",
    "    train = knn_Impute(train, numeric_cols, cat_cols, reverse_dummy = True)\n",
    "    if verbose: print(\"4. Imputed missing values using knn with k = 5\")\n",
    "\n",
    "    # Create dummy variable\n",
    "    train = dummy_cols(train, cat_cols)\n",
    "    if verbose: print(\"5. Categorical columns were convert into n - 1 binary dummy variables\")\n",
    "\n",
    "    # Yeo-Johnson on Numerics\n",
    "    train = yeo_johnson(train, numeric_cols)\n",
    "    if verbose: print('6. Yeo-Johnson Transformation of numeric columns')\n",
    "\n",
    "    # Standardized\n",
    "    train = standardize(train, numeric_cols)\n",
    "    if verbose: print('7. Numeric columns scaled to mean 0 and unit variance')\n",
    "\n",
    "    # Splines\n",
    "    train = add_ns_3(train, cols = numeric_cols)\n",
    "    if verbose: print('8. Numeric features transformed into natural cubic splines')\n",
    "\n",
    "    # Add log Price back in \n",
    "    if rep: \n",
    "        train.insert(loc = 0, column = 'SalePrice', value = np.log(train_rep))\n",
    "        if verbose: print(\"9. Log transformation of response\")\n",
    "\n",
    "    # Add Ids back in\n",
    "    train.insert(loc = 0, column = 'Id', value = train_Id)\n",
    "\n",
    "    return(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pLR = recipe_pLR(train_raw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run penalized regression model \n",
    "penalty_type = [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1]\n",
    "Y = train_pLR['SalePrice']\n",
    "X = train_pLR.drop(['SalePrice','Id'], axis = 1)\n",
    "pLR_model = ElasticNetCV(l1_ratio = penalty_type, cv = 10, \n",
    "    verbose = 1, random_state = 123) #123 is the seed \n",
    "pLR_fit = pLR_model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hyper-parameters (Lasso with small penalty)\n",
    "optimal_type = pLR_model.l1_ratio_\n",
    "optimal_pen = pLR_model.alpha_\n",
    "\n",
    "#Extract Coefficients\n",
    "coef = pd.DataFrame(pLR_model.coef_, index = X.columns, columns = [\"Beta\"])\n",
    "coef = coef[coef['Beta'] > 0]\n",
    "coef.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "pLR_train_pred = np.exp(pLR_model.predict(X))\n",
    "pLR_train_rmse = mean_squared_error(np.exp(Y), pLR_train_pred, squared = False)\n",
    "print(pLR_train_rmse)\n",
    "\n",
    "# Make predictions on testing data\n",
    "\n",
    "# Cleaning Testing data\n",
    "test_pLR = recipe_pLR(test_raw, rep = False)\n",
    "\n",
    "# Match training a testing columns\n",
    "# Match Drops \n",
    "train_drops = np.setdiff1d(test_pLR.columns, train_pLR.columns)\n",
    "train_drop = train_drops.tolist()\n",
    "X_test = test_pLR.drop(train_drops, axis = 1)\n",
    "# Add 0's for missing factor levels \n",
    "mis_levels = np.setdiff1d(X.columns, X_test.columns)\n",
    "mis_levels.tolist()\n",
    "X_test[mis_levels] = 0\n",
    "X_test = X_test.drop(['Id'], axis = 1)\n",
    "# Match feature orders\n",
    "X_test = X_test.reindex(X.columns, axis = 1)\n",
    "\n",
    "# Apply model \n",
    "pLR_predictions = np.exp(pLR_model.predict(X_test))\n",
    "\n",
    "#Submission Format\n",
    "kaggle_pLR = pd.DataFrame()\n",
    "kaggle_pLR['Id'] = test_raw['Id']\n",
    "kaggle_pLR['SalePrice'] = pLR_predictions\n",
    "\n",
    "kaggle_pLR.to_csv(\"pLR_Kaggle_Submission_final.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recipe_RF(train, verbose = True, rep = True):\n",
    "    if verbose: print('Preprocessing Steps: ')\n",
    "    # Drop Id\n",
    "    if 'Id' in train: \n",
    "        train_Id = train['Id'] \n",
    "        train = train.drop('Id', axis = 1)\n",
    "\n",
    "    # Drop Response\n",
    "    if 'SalePrice' in train: \n",
    "        train_rep = train['SalePrice']\n",
    "        train = train.drop('SalePrice', axis = 1)\n",
    "\n",
    "    # Add user features\n",
    "    train = feat_eng(train)\n",
    "    if verbose: print(\"1. Added user engineered features\")\n",
    "\n",
    "    # Marks columns as categorical \n",
    "    train = to_cat(train)\n",
    "    train = some_num_to_cat(train)\n",
    "    if verbose: print('2. Encoded user specified variables as categorical')\n",
    "\n",
    "    # Ordinarily encodes select variables\n",
    "    train = ord_encode(train)\n",
    "    if verbose: print(\"3. Encoded user specified variables to be ordinal\")\n",
    "\n",
    "    # Track which variables are numeric and categorical \n",
    "    numeric_cols = train.select_dtypes(include = np.number).columns\n",
    "    cat_cols = train.select_dtypes('category').columns\n",
    "\n",
    "    # Imputes missing values\n",
    "    train = knn_Impute(train, numeric_cols, cat_cols, reverse_dummy = True)\n",
    "    if verbose: print(\"4. Imputed missing values using knn with k = 5\")\n",
    "\n",
    "    # Create dummy variable\n",
    "    train = dummy_cols(train, cat_cols, drop_first = False)\n",
    "    if verbose: print(\"5. Categorical columns were convert into n binary dummy variables\")\n",
    "\n",
    "    # Standardized\n",
    "    train = standardize(train, numeric_cols)\n",
    "    if verbose: print('6. Numeric columns scaled to mean 0 and unit variance')\n",
    "\n",
    "    # Add Price back in \n",
    "    if rep: \n",
    "        train.insert(loc = 0, column = 'SalePrice', value = train_rep)\n",
    "\n",
    "    # Add Ids back in\n",
    "    train.insert(loc = 0, column = 'Id', value = train_Id)\n",
    "\n",
    "    return(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_RF = recipe_RF(train_raw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import random forest package.\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a random grid of tuning parameters for the RF model.\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit 100 random combinations of parameters from the grid. \n",
    "rf = RandomForestRegressor()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
    "    n_iter = 50, cv = 5, verbose = 2, random_state = 123, n_jobs = 30) \n",
    "        # Note: n_jobs indicates how many instances to run in parallel. \n",
    "        # Error occurs when trying to use over 30 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the random models\n",
    "Y = train_RF['SalePrice']\n",
    "X = train_RF.drop(['SalePrice','Id'], axis = 1)\n",
    "rf_random.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best random model parameters\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a parameter grid for a grid search based on the random search results.\n",
    "param_grid = {'n_estimators': [1200, 1300, 1400, 1500, 1600],\n",
    "               'max_features': ['sqrt'],\n",
    "               'max_depth': [40, 45, 50, 55, 60],\n",
    "               'min_samples_split': [2, 3, 4, 5, 6, 7],\n",
    "               'min_samples_leaf': [1],\n",
    "               'bootstrap': [False]}\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "rf_grid = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "    cv = 5, n_jobs = 30, verbose = 2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models based on grid search \n",
    "rf_grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Best Model\n",
    "rf_optimal = rf_grid.best_estimator_\n",
    "\n",
    "# Training RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "rf_train_pred = rf_optimal.predict(X)\n",
    "rf_train_rmse = mean_squared_error(Y, rf_train_pred, squared = False)\n",
    "print(rf_train_rmse)\n",
    "\n",
    "# Make predictions on testing data\n",
    "\n",
    "# Cleaning Testing data\n",
    "test_rf = recipe_RF(test_raw, rep = False)\n",
    "\n",
    "train_rf = recipe_RF(train_raw)\n",
    "\n",
    "# Match training a testing columns\n",
    "# Match Drops \n",
    "train_drops = np.setdiff1d(test_rf.columns, train_rf.columns)\n",
    "train_drop = train_drops.tolist()\n",
    "X_test = test_rf.drop(train_drops, axis = 1)\n",
    "# Add 0's for missing factor levels \n",
    "mis_levels = np.setdiff1d(X.columns, X_test.columns)\n",
    "mis_levels.tolist()\n",
    "X_test[mis_levels] = 0\n",
    "X_test = X_test.drop(['Id'], axis = 1)\n",
    "# Match feature orders\n",
    "X_test = X_test.reindex(X.columns, axis = 1)\n",
    "\n",
    "# Apply model \n",
    "rf_test_pred = rf_optimal.predict(X_test)\n",
    "\n",
    "#Submission Format\n",
    "kaggle_rf = pd.DataFrame()\n",
    "kaggle_rf['Id'] = test_raw['Id']\n",
    "kaggle_rf['SalePrice'] = rf_test_pred\n",
    "\n",
    "kaggle_rf.to_csv(\"rf_Kaggle_Submission_final.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the same data cleaning as the RF model.\n",
    "recipe_XGB = recipe_RF\n",
    "train_XGB = recipe_XGB(train_raw)\n",
    "\n",
    "Y = train_XGB['SalePrice']\n",
    "X = train_XGB.drop(['SalePrice','Id'], axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB Packages\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "xgb = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a random grid of tuning parameters for the XBG model.\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Learning rate of the weak learner\n",
    "learning_rate = [0.001, 0.01, 0.01]\n",
    "\n",
    "# Type of loss function to optimize\n",
    "loss = ['squared_error', 'absolute_error']\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'learning_rate': learning_rate,\n",
    "               'loss': loss}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits 250 model based on random samples from the above grid.\n",
    "xgb_random = RandomizedSearchCV(estimator = xgb, param_distributions = random_grid, \n",
    "    n_iter = 50, cv = 5, verbose = 2, random_state = 123, n_jobs = 30) \n",
    "\n",
    "xgb_random.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Parameters based on a random search\n",
    "xgb_random.best_params_\n",
    "\n",
    "# Define a new grid to conduct an exhaustive search \n",
    "xgb_grid_search = {'n_estimators': [700, 800, 900],\n",
    "                   'max_depth': [55, 60, 65],\n",
    "                   'min_samples_split': [1, 2, 3],\n",
    "                   'learning_rate': [0.005, 0.01, 0.015],\n",
    "                   'loss': ['absolute_error']}\n",
    "\n",
    "xgb_grid = GridSearchCV(estimator = xgb, param_grid = xgb_grid_search, \n",
    "    cv = 5, n_jobs = 30, verbose = 2, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Best Model\n",
    "xgb_optimal = xgb_grid.best_estimator_\n",
    "\n",
    "# Training RMSE\n",
    "xgb_train_pred = xgb_optimal.predict(X)\n",
    "xgb_train_rmse = mean_squared_error(Y, xgb_train_pred, squared = False)\n",
    "print(xgb_train_rmse)\n",
    "\n",
    "# Make predictions on testing data\n",
    "\n",
    "# Cleaning Testing data\n",
    "test_xgb = recipe_XGB(test_raw, rep = False)\n",
    "\n",
    "train_xgb = recipe_XGB(train_raw)\n",
    "\n",
    "# Match training a testing columns\n",
    "# Match Drops \n",
    "train_drops = np.setdiff1d(test_xgb.columns, train_xgb.columns)\n",
    "train_drop = train_drops.tolist()\n",
    "X_test = test_xgb.drop(train_drops, axis = 1)\n",
    "# Add 0's for missing factor levels \n",
    "mis_levels = np.setdiff1d(X.columns, X_test.columns)\n",
    "mis_levels.tolist()\n",
    "X_test[mis_levels] = 0\n",
    "X_test = X_test.drop(['Id'], axis = 1)\n",
    "# Match feature orders\n",
    "X_test = X_test.reindex(X.columns, axis = 1)\n",
    "\n",
    "# Apply model \n",
    "xgb_test_pred = xgb_optimal.predict(X_test)\n",
    "\n",
    "#Submission Format\n",
    "kaggle_xgb = pd.DataFrame()\n",
    "kaggle_xgb['Id'] = test_raw['Id']\n",
    "kaggle_xgb['SalePrice'] = xgb_test_pred\n",
    "\n",
    "kaggle_xgb.to_csv(\"xgb_Kaggle_Submission_final.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neutral Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neutral Network Packages\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import LeakyReLU\n",
    "import keras_tuner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "# Defines data cleaning steps for the NN models\n",
    "# No Feature engineering \n",
    "# No adjustments for skewness \n",
    "# No splines\n",
    "# No log Response\n",
    "def recipe_NN(train, verbose = True, rep = True): \n",
    "    if verbose: print('Preprocessing Steps: ')\n",
    "    # Drop Id\n",
    "    if 'Id' in train: \n",
    "        train_Id = train['Id'] \n",
    "        train = train.drop('Id', axis = 1)\n",
    "\n",
    "    # Drop Response\n",
    "    if 'SalePrice' in train: \n",
    "        train_rep = train['SalePrice']\n",
    "        train = train.drop('SalePrice', axis = 1)\n",
    "\n",
    "    # Marks columns as categorical \n",
    "    train = to_cat(train)\n",
    "    train = some_num_to_cat(train)\n",
    "    if verbose: print('1. Encoded user specified variables as categorical')\n",
    "\n",
    "    # Ordinarily encodes select variables\n",
    "    train = ord_encode(train)\n",
    "    if verbose: print(\"2. Encoded user specified variables to be ordinal\")\n",
    "\n",
    "    # Track which variables are numeric and categorical \n",
    "    numeric_cols = train.select_dtypes(include = np.number).columns\n",
    "    cat_cols = train.select_dtypes('category').columns\n",
    "\n",
    "    # Imputes missing values\n",
    "    train = knn_Impute(train, numeric_cols, cat_cols, reverse_dummy = True)\n",
    "    if verbose: print(\"3. Imputed missing values using knn with k = 5\")\n",
    "\n",
    "    # Create dummy variable\n",
    "    train = dummy_cols(train, cat_cols, drop_first = False)\n",
    "    if verbose: print(\"4. Categorical columns were convert into n binary dummy variables\")\n",
    "\n",
    "    # Standardized\n",
    "    train = standardize(train, numeric_cols)\n",
    "    if verbose: print('5. Numeric columns scaled to mean 0 and unit variance')\n",
    "\n",
    "    # Add log Price back in \n",
    "    if rep: \n",
    "        train.insert(loc = 0, column = 'SalePrice', value = train_rep)\n",
    "\n",
    "    # Add Ids back in\n",
    "    train.insert(loc = 0, column = 'Id', value = train_Id)\n",
    "\n",
    "    return(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Steps: \n",
      "1. Encoded user specified variables as categorical\n",
      "2. Encoded user specified variables to be ordinal\n",
      "3. Imputed missing values using knn with k = 5\n",
      "4. Categorical columns were convert into n binary dummy variables\n",
      "5. Numeric columns scaled to mean 0 and unit variance\n"
     ]
    }
   ],
   "source": [
    "# Apply cleaning recipe\n",
    "train_NN_clean = recipe_NN(train_raw)\n",
    "X = train_NN_clean.drop(['Id', 'SalePrice'], axis = 1)\n",
    "Y = train_NN_clean['SalePrice']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x26df9ad4430>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    # Input Layer\n",
    "    model.add(keras.Input(shape = (531, )))\n",
    "\n",
    "    # Tune the number of hidden layers\n",
    "    for i in range(hp.Int(\"num_layers\", min_value = 1, max_value = 3, step = 1)):\n",
    "        model.add(\n",
    "            # Tune the number of nodes in each layer\n",
    "            layers.Dense(\n",
    "                units = hp.Int(f\"units{i}\", min_value = 5, max_value = 30, step = 5), \n",
    "                # Tune activation function between layers\n",
    "                activation = hp.Choice(\"activation\", [\"relu\", \"tanh\", \"LeakyReLU\", \"linear\"]),\n",
    "                    #Note: I believe None = linear activation\n",
    "                # Tune weight penalty\n",
    "                kernel_regularizer = keras.regularizers.L1L2(\n",
    "                    l1 = hp.Float(\"lasso\", min_value = 0, max_value = 2, step = 0.1), \n",
    "                    l2 = hp.Float(\"ridge\", min_value = 0, max_value = 2, step = 0.1)     \n",
    "                ),\n",
    "                # Tune bias penalty\n",
    "                bias_regularizer = keras.regularizers.L1L2(\n",
    "                    l1 = hp.Float(\"lasso\", min_value = 0, max_value = 2, step = 0.1), \n",
    "                    l2 = hp.Float(\"ridge\", min_value = 0, max_value = 2, step = 0.1) \n",
    "                ), \n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Tune Dropout rate\n",
    "    model.add(layers.Dropout(rate = hp.Float(\"Dropout\", min_value = 0, max_value = 0.5, step = 0.1)))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(1, activation = 'relu'))\n",
    "\n",
    "    # Learning rate schedule with decay. \n",
    "    learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "        boundaries = [500], values = [0.01, 0.001])\n",
    "\n",
    "    # Model Compiler\n",
    "    model.compile(\n",
    "        loss = keras.losses.MeanSquaredError(), \n",
    "\n",
    "        # Tune optimization \n",
    "        # SGD caused nan RMSE\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate = learning_rate_fn, \n",
    "            momentum = hp.Float(\"momentum\", min_value = 0, max_value = 0.5, step = 0.1)\n",
    "        ), \n",
    "\n",
    "        metrics = [keras.metrics.RootMeanSquaredError(name = 'rmse')]\n",
    "    )\n",
    "\n",
    "    return model \n",
    "\n",
    "build_model(keras_tuner.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 7\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 3, 'step': 1, 'sampling': None}\n",
      "units0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 5, 'max_value': 30, 'step': 5, 'sampling': None}\n",
      "activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'LeakyReLU', 'linear'], 'ordered': False}\n",
      "lasso (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 2.0, 'step': 0.1, 'sampling': None}\n",
      "ridge (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 2.0, 'step': 0.1, 'sampling': None}\n",
      "Dropout (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.1, 'sampling': None}\n",
      "momentum (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.1, 'sampling': None}\n"
     ]
    }
   ],
   "source": [
    "# Random Sample Grid of Hyperparameters\n",
    "\n",
    "tuner = keras_tuner.RandomSearch(\n",
    "    hypermodel = build_model, \n",
    "    max_trials = 5, \n",
    "    executions_per_trial = 2,\n",
    "    objective = keras_tuner.Objective('rmse', direction = 'min'), \n",
    "    overwrite = True,\n",
    "    seed = 123, \n",
    "    directory = \"NN_Tuning_2\",\n",
    "    project_name = \"test_rand_final_3\"\n",
    ")\n",
    "\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 01m 40s]\n",
      "rmse: 196503.4375\n",
      "\n",
      "Best rmse So Far: 37657.7109375\n",
      "Total elapsed time: 00h 07m 42s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(X, Y, epochs = 1000, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 15)                7980      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                160       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 15)                165       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 15)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,321\n",
      "Trainable params: 8,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Extract Best Model\n",
    "model_best = tuner.get_best_models(num_models = 1)\n",
    "model_best[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in NN_Tuning_2\\test_rand_final_2\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x0000026DFA9A2DF0>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units0: 15\n",
      "activation: linear\n",
      "lasso: 1.1\n",
      "ridge: 0.7000000000000001\n",
      "Dropout: 0.30000000000000004\n",
      "momentum: 0.30000000000000004\n",
      "units1: 10\n",
      "units2: 15\n",
      "Score: 37657.7109375\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units0: 20\n",
      "activation: LeakyReLU\n",
      "lasso: 0.6000000000000001\n",
      "ridge: 1.7000000000000002\n",
      "Dropout: 0.4\n",
      "momentum: 0.5\n",
      "units1: 5\n",
      "units2: 20\n",
      "Score: 43132.853515625\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units0: 25\n",
      "activation: linear\n",
      "lasso: 0.30000000000000004\n",
      "ridge: 0.9\n",
      "Dropout: 0.30000000000000004\n",
      "momentum: 0.0\n",
      "units1: 5\n",
      "units2: 5\n",
      "Score: 55659.349609375\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units0: 15\n",
      "activation: linear\n",
      "lasso: 1.7000000000000002\n",
      "ridge: 1.4000000000000001\n",
      "Dropout: 0.4\n",
      "momentum: 0.30000000000000004\n",
      "units1: 30\n",
      "units2: 5\n",
      "Score: 66783.90625\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 1\n",
      "units0: 20\n",
      "activation: tanh\n",
      "lasso: 1.9000000000000001\n",
      "ridge: 1.4000000000000001\n",
      "Dropout: 0.2\n",
      "momentum: 0.1\n",
      "units1: 25\n",
      "units2: 20\n",
      "Score: 196503.4375\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Steps: \n",
      "1. Encoded user specified variables as categorical\n",
      "2. Encoded user specified variables to be ordinal\n",
      "3. Imputed missing values using knn with k = 5\n",
      "4. Categorical columns were convert into n binary dummy variables\n",
      "5. Numeric columns scaled to mean 0 and unit variance\n",
      "46/46 [==============================] - 0s 757us/step\n"
     ]
    }
   ],
   "source": [
    "# Predicting on Test Data\n",
    "\n",
    "# Cleaning Testing data for NN\n",
    "test_NN = recipe_NN(test_raw, rep = False)\n",
    "\n",
    "# Match training a testing columns\n",
    "# Match Drops \n",
    "train_drops = np.setdiff1d(test_NN.columns, train_NN_clean.columns)\n",
    "train_drop = train_drops.tolist()\n",
    "X_test = test_NN.drop(train_drops, axis = 1)\n",
    "\n",
    "# Add 0's for missing factor levels \n",
    "mis_levels = np.setdiff1d(X.columns, X_test.columns)\n",
    "mis_levels.tolist()\n",
    "X_test[mis_levels] = 0\n",
    "X_test = X_test.drop(['Id'], axis = 1)\n",
    "\n",
    "# Match feature orders\n",
    "X_test = X_test.reindex(X.columns, axis = 1)\n",
    "\n",
    "NN_pred_reduced = model_best[0].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_NN_2 = pd.DataFrame()\n",
    "kaggle_NN_2['Id'] = test_raw['Id']\n",
    "kaggle_NN_2['SalePrice'] = NN_pred_reduced\n",
    "\n",
    "kaggle_NN_2.to_csv(\"NN_Kaggle_Submission_reduced.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x15767b20b50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce the hyperparameter space\n",
    "def build_model_2(hp):\n",
    "    model = keras.Sequential()\n",
    "    # Input Layer\n",
    "    model.add(keras.Input(shape = (531, )))\n",
    "\n",
    "    # Tune the number of hidden layers\n",
    "    for i in range(hp.Int(\"num_layers\", min_value = 2, max_value = 4, step = 1)):\n",
    "        model.add(\n",
    "            # Tune the number of nodes in each layer\n",
    "            layers.Dense(\n",
    "                units = hp.Int(f\"units{i}\", min_value = 20, max_value = 50, step = 5), \n",
    "                # Tune activation function between layers\n",
    "                activation = hp.Choice(\"activation\", [\"relu\", \"LeakyReLU\", \"linear\"]),\n",
    "                    #Note: I believe None = linear activation\n",
    "                # Tune weight penalty\n",
    "                kernel_regularizer = keras.regularizers.L1L2(\n",
    "                    l1 = hp.Float(\"lasso\", min_value = 0, max_value = 5, step = 0.5), \n",
    "                    l2 = hp.Float(\"ridge\", min_value = 0, max_value = 5, step = 0.5)     \n",
    "                ),\n",
    "                # Tune bias penalty\n",
    "                bias_regularizer = keras.regularizers.L1L2(\n",
    "                    l1 = hp.Float(\"lasso\", min_value = 0, max_value = 5, step = 0.5), \n",
    "                    l2 = hp.Float(\"ridge\", min_value = 0, max_value = 5, step = 0.5) \n",
    "                ), \n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Tune Dropout rate\n",
    "    model.add(layers.Dropout(rate = hp.Float(\"Dropout\", min_value = 0, max_value = 0.5, step = 0.1)))\n",
    "\n",
    "    # Output Layer\n",
    "    model.add(layers.Dense(1, activation = 'relu'))\n",
    "\n",
    "    # Learning rate schedule with decay. \n",
    "    learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "        boundaries = [500], values = [0.01, 0.001])\n",
    "\n",
    "    # Model Compiler\n",
    "    model.compile(\n",
    "        loss = keras.losses.MeanSquaredError(), \n",
    "\n",
    "        # Tune optimization \n",
    "        # SGD caused nan RMSE\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate = learning_rate_fn, \n",
    "            momentum = hp.Float(\"momentum\", min_value = 0, max_value = 0.5, step = 0.1)\n",
    "        ), \n",
    "\n",
    "        metrics = [keras.metrics.RootMeanSquaredError(name = 'rmse')]\n",
    "    )\n",
    "\n",
    "    return model \n",
    "\n",
    "build_model_2(keras_tuner.HyperParameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 8\n",
      "num_layers (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 2, 'max_value': 4, 'step': 1, 'sampling': None}\n",
      "units0 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 20, 'max_value': 50, 'step': 5, 'sampling': None}\n",
      "activation (Choice)\n",
      "{'default': 'relu', 'conditions': [], 'values': ['relu', 'LeakyReLU', 'linear'], 'ordered': False}\n",
      "lasso (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 5.0, 'step': 0.5, 'sampling': None}\n",
      "ridge (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 5.0, 'step': 0.5, 'sampling': None}\n",
      "units1 (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 20, 'max_value': 50, 'step': 5, 'sampling': None}\n",
      "Dropout (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.1, 'sampling': None}\n",
      "momentum (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.1, 'sampling': None}\n"
     ]
    }
   ],
   "source": [
    "# Hyperband Grid of Hyperparameters\n",
    "\n",
    "tuner_2 = keras_tuner.Hyperband(\n",
    "    hypermodel = build_model_2, \n",
    "    objective = \"val_loss\", \n",
    "    max_epochs = 1000, \n",
    "    overwrite = True,\n",
    "    seed = 123,\n",
    "    directory = \"Hyperband\",\n",
    "    project_name = \"test_hyband_9\"\n",
    ")\n",
    "\n",
    "tuner_2.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # creates unique file names based on system time\n",
    "\n",
    "# Define early stop function\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5)\n",
    "\n",
    "# Define Tensorboard log \n",
    "Board_Log_Name = f\"Tensorboard_Log_{int(time.time())}\"\n",
    "tensorboard = TensorBoard(log_dir=f\"logs/{Board_Log_Name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2074 Complete [00h 00m 03s]\n",
      "val_loss: 596423360.0\n",
      "\n",
      "Best val_loss So Far: 427372384.0\n",
      "Total elapsed time: 01h 01m 28s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# Apply Hyperband Algo\n",
    "tuner_2.search(X, Y, epochs = 1000, validation_split = 0.1, \n",
    "    callbacks = [stop_early, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in Hyperband\\test_hyband_9\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x000001570F7AC970>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units0: 25\n",
      "activation: relu\n",
      "lasso: 4.0\n",
      "ridge: 4.0\n",
      "units1: 35\n",
      "Dropout: 0.30000000000000004\n",
      "momentum: 0.30000000000000004\n",
      "units2: 40\n",
      "units3: 25\n",
      "tuner/epochs: 38\n",
      "tuner/initial_epoch: 13\n",
      "tuner/bracket: 5\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 1584\n",
      "Score: 427372384.0\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units0: 30\n",
      "activation: relu\n",
      "lasso: 3.5\n",
      "ridge: 2.0\n",
      "units1: 20\n",
      "Dropout: 0.1\n",
      "momentum: 0.4\n",
      "units2: 40\n",
      "units3: 30\n",
      "tuner/epochs: 334\n",
      "tuner/initial_epoch: 112\n",
      "tuner/bracket: 3\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 1989\n",
      "Score: 450141888.0\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units0: 30\n",
      "activation: relu\n",
      "lasso: 1.0\n",
      "ridge: 0.0\n",
      "units1: 40\n",
      "Dropout: 0.2\n",
      "momentum: 0.4\n",
      "units2: 25\n",
      "units3: 45\n",
      "tuner/epochs: 112\n",
      "tuner/initial_epoch: 38\n",
      "tuner/bracket: 5\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 1686\n",
      "Score: 450942592.0\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units0: 30\n",
      "activation: relu\n",
      "lasso: 1.0\n",
      "ridge: 1.5\n",
      "units1: 50\n",
      "Dropout: 0.2\n",
      "momentum: 0.30000000000000004\n",
      "units2: 30\n",
      "units3: 45\n",
      "tuner/epochs: 112\n",
      "tuner/initial_epoch: 38\n",
      "tuner/bracket: 4\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 1888\n",
      "Score: 452319776.0\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units0: 45\n",
      "activation: relu\n",
      "lasso: 1.0\n",
      "ridge: 4.5\n",
      "units1: 30\n",
      "Dropout: 0.0\n",
      "momentum: 0.2\n",
      "units2: 45\n",
      "units3: 40\n",
      "tuner/epochs: 112\n",
      "tuner/initial_epoch: 38\n",
      "tuner/bracket: 5\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 1689\n",
      "Score: 455298912.0\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 3\n",
      "units0: 35\n",
      "activation: relu\n",
      "lasso: 0.0\n",
      "ridge: 1.5\n",
      "units1: 30\n",
      "Dropout: 0.30000000000000004\n",
      "momentum: 0.30000000000000004\n",
      "units2: 45\n",
      "units3: 35\n",
      "tuner/epochs: 112\n",
      "tuner/initial_epoch: 38\n",
      "tuner/bracket: 5\n",
      "tuner/round: 3\n",
      "tuner/trial_id: 1717\n",
      "Score: 455362112.0\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units0: 30\n",
      "activation: relu\n",
      "lasso: 3.5\n",
      "ridge: 2.0\n",
      "units1: 20\n",
      "Dropout: 0.1\n",
      "momentum: 0.4\n",
      "units2: 40\n",
      "units3: 30\n",
      "tuner/epochs: 38\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 3\n",
      "tuner/round: 0\n",
      "Score: 457097088.0\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units0: 50\n",
      "activation: relu\n",
      "lasso: 2.0\n",
      "ridge: 2.0\n",
      "units1: 45\n",
      "Dropout: 0.1\n",
      "momentum: 0.1\n",
      "units2: 45\n",
      "units3: 40\n",
      "tuner/epochs: 112\n",
      "tuner/initial_epoch: 38\n",
      "tuner/bracket: 6\n",
      "tuner/round: 4\n",
      "tuner/trial_id: 1224\n",
      "Score: 457701600.0\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units0: 50\n",
      "activation: relu\n",
      "lasso: 4.5\n",
      "ridge: 3.5\n",
      "units1: 40\n",
      "Dropout: 0.4\n",
      "momentum: 0.2\n",
      "units2: 25\n",
      "units3: 35\n",
      "tuner/epochs: 112\n",
      "tuner/initial_epoch: 38\n",
      "tuner/bracket: 4\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 1882\n",
      "Score: 458495680.0\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "num_layers: 4\n",
      "units0: 45\n",
      "activation: relu\n",
      "lasso: 1.5\n",
      "ridge: 1.5\n",
      "units1: 20\n",
      "Dropout: 0.2\n",
      "momentum: 0.30000000000000004\n",
      "units2: 35\n",
      "units3: 40\n",
      "tuner/epochs: 334\n",
      "tuner/initial_epoch: 112\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 2031\n",
      "Score: 459742784.0\n"
     ]
    }
   ],
   "source": [
    "tuner_2.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.momentum\n",
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.rho\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 25)                13300     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 35)                910       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 40)                1440      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 25)                1025      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 25)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 26        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,701\n",
      "Trainable params: 16,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Extract Best Model\n",
    "model_best_2 = tuner_2.get_best_models(num_models = 1)\n",
    "model_best_2[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tuner_2 object\n",
    "import pickle\n",
    "with open('data/tuner_2.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(tuner_2, pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('data/tuner_2.pkl', 'rb') as pickle_file:\n",
    "    #tuner_2_reload = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Steps: \n",
      "1. Encoded user specified variables as categorical\n",
      "2. Encoded user specified variables to be ordinal\n",
      "3. Imputed missing values using knn with k = 5\n",
      "4. Categorical columns were convert into n binary dummy variables\n",
      "5. Numeric columns scaled to mean 0 and unit variance\n",
      "46/46 [==============================] - 0s 622us/step\n"
     ]
    }
   ],
   "source": [
    "# Predicting on Test Data\n",
    "\n",
    "model_best_2 = tuner_2.get_best_models(1)\n",
    "\n",
    "# Cleaning Testing data for NN\n",
    "test_NN = recipe_NN(test_raw, rep = False)\n",
    "\n",
    "# Match training a testing columns\n",
    "# Match Drops \n",
    "train_drops = np.setdiff1d(test_NN.columns, train_NN_clean.columns)\n",
    "train_drop = train_drops.tolist()\n",
    "X_test = test_NN.drop(train_drops, axis = 1)\n",
    "\n",
    "# Add 0's for missing factor levels \n",
    "mis_levels = np.setdiff1d(X.columns, X_test.columns)\n",
    "mis_levels.tolist()\n",
    "X_test[mis_levels] = 0\n",
    "X_test = X_test.drop(['Id'], axis = 1)\n",
    "\n",
    "# Match feature orders\n",
    "X_test = X_test.reindex(X.columns, axis = 1)\n",
    "\n",
    "NN_pred_tuned = model_best_2[0].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_NN_tuned = pd.DataFrame()\n",
    "kaggle_NN_tuned['Id'] = test_raw['Id']\n",
    "kaggle_NN_tuned['SalePrice'] = NN_pred_tuned\n",
    "\n",
    "kaggle_NN_tuned.to_csv(\"NN_Kaggle_Submission_final.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "2043299c89c8cd0b4d1a6f5cf4529bd58e6a4e0fe3181a25e0d328c821cdc5c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
